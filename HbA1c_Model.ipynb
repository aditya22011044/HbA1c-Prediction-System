{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1defb1",
   "metadata": {},
   "source": [
    "# Data fetching for HbA1c\n",
    " Imputing mean for Hba1c missing values with latest available values for other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b102376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Define number of patients to limit the processing\n",
    "num_patients = 10000\n",
    "\n",
    "# Step 2: Establish DB Connection\n",
    "# IMPORTANT: Replace with your actual database host, user, password, and database name\n",
    "try:\n",
    "    conn = mysql.connector.connect(\n",
    "        host='127.0.0.1',\n",
    "        user='root',\n",
    "        password='Omkar@123',\n",
    "        database='ehr_raw_data6_7'\n",
    "    )\n",
    "    if conn.is_connected():\n",
    "        print(\"Successfully connected to the database!\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error connecting to the database: {err}\")\n",
    "    # Exit or handle the error appropriately if connection fails\n",
    "    exit() # Exiting for this example, consider a more robust error handling\n",
    "\n",
    "# Step 3: Load relevant data from the database\n",
    "# The query is updated to fetch required columns and filter by the new date range.\n",
    "query = \"\"\"\n",
    "SELECT patient_id, visit_date, age, gender, cci_score, test_name, result_value,\n",
    "        bmi, blood_pressure_systolic, blood_pressure_diastolic, blood_glucose_level\n",
    "FROM ehr_feature\n",
    "WHERE visit_date BETWEEN '2023-08-01' AND '2025-07-31'\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# Close the database connection as soon as data is fetched\n",
    "if 'conn' in locals() and conn.is_connected():\n",
    "    conn.close()\n",
    "    print(\"Database connection closed.\")\n",
    "\n",
    "# Ensure correct data types after loading from SQL\n",
    "df['visit_date'] = pd.to_datetime(df['visit_date'])\n",
    "# Use 'Int64' for integer column that might contain NaNs (nulls from DB)\n",
    "df['cci_score'] = pd.to_numeric(df['cci_score'], errors='coerce').astype('Int64')\n",
    "# Ensure other numerical columns are also appropriate types if needed\n",
    "df['result_value'] = pd.to_numeric(df['result_value'], errors='coerce')\n",
    "df['bmi'] = pd.to_numeric(df['bmi'], errors='coerce')\n",
    "df['blood_pressure_systolic'] = pd.to_numeric(df['blood_pressure_systolic'], errors='coerce')\n",
    "df['blood_pressure_diastolic'] = pd.to_numeric(df['blood_pressure_diastolic'], errors='coerce')\n",
    "df['blood_glucose_level'] = pd.to_numeric(df['blood_glucose_level'], errors='coerce')\n",
    "\n",
    "\n",
    "# Step 4: Limit to the first `num_patients`\n",
    "# This ensures processing is limited to a specific number of unique patients.\n",
    "df = df.sort_values('patient_id')\n",
    "unique_patients = df['patient_id'].unique()[:num_patients]\n",
    "df = df[df['patient_id'].isin(unique_patients)].copy()\n",
    "\n",
    "\n",
    "# Step 5: Create monthly HbA1c pivot for the new date range (July 2023 - June 2025)\n",
    "# Filter the DataFrame to include only 'HbA1c' test results.\n",
    "hba1c_df = df[df['test_name'] == 'HbA1c'].copy()\n",
    "\n",
    "# Create a 'year_month' column for pivoting, in 'monthname_YYYY' format\n",
    "hba1c_df['year_month'] = hba1c_df['visit_date'].dt.strftime('%B_%Y').str.lower()\n",
    "\n",
    "# Sort data to identify the most recent 'result_value' for each patient within each year_month.\n",
    "hba1c_df = hba1c_df.sort_values(by=['patient_id', 'visit_date'], ascending=[True, False]) # Sort by visit_date descending to get latest\n",
    "\n",
    "# Remove duplicates within each patient-year_month group, keeping the most recent\n",
    "latest_hba1c_df = hba1c_df.drop_duplicates(subset=['patient_id', 'year_month'], keep='first')\n",
    "\n",
    "# Select only the required columns for pivoting\n",
    "latest_hba1c_df = latest_hba1c_df[['patient_id', 'year_month', 'result_value']]\n",
    "\n",
    "# Pivot the table to get 'year_month' as columns and HbA1c results as values.\n",
    "hba1c_pivot = latest_hba1c_df.pivot(index='patient_id', columns='year_month', values='result_value')\n",
    "\n",
    "# Generate a list of all expected 'monthname_YYYY' months from August 2023 to July 2025\n",
    "start_date = datetime(2023, 8, 1)\n",
    "end_date = datetime(2025, 7, 31)\n",
    "current_date = start_date\n",
    "expected_months = []\n",
    "while current_date <= end_date:\n",
    "    expected_months.append(current_date.strftime('%B_%Y').lower()) # Changed format to monthname_YYYY_lower\n",
    "    # Move to the next month\n",
    "    if current_date.month == 12:\n",
    "        current_date = current_date.replace(year=current_date.year + 1, month=1)\n",
    "    else:\n",
    "        current_date = current_date.replace(month=current_date.month + 1)\n",
    "\n",
    "# Reindex the pivot table to ensure all expected months are present, filling missing with NaN.\n",
    "hba1c_pivot = hba1c_pivot.reindex(columns=expected_months)\n",
    "\n",
    "# Rename the month columns to the desired format (hba1c_monthname_YYYY).\n",
    "hba1c_pivot.columns = [f\"hba1c_{col}\" for col in hba1c_pivot.columns]\n",
    "\n",
    "# Step 6: Extract other features by prioritizing the latest available month for each feature\n",
    "print(\"Extracting other features by prioritizing latest available month for each feature...\")\n",
    "\n",
    "other_feature_cols_to_extract = [\n",
    "    'age', 'gender', 'bmi', 'blood_pressure_systolic', 'blood_pressure_diastolic',\n",
    "    'blood_glucose_level', 'cci_score'\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame to store the final other features\n",
    "final_other_features = pd.DataFrame({'patient_id': unique_patients})\n",
    "\n",
    "# Sort the main DataFrame by patient_id and visit_date (descending)\n",
    "df_sorted_by_date = df.sort_values(by=['patient_id', 'visit_date'], ascending=[True, False])\n",
    "\n",
    "# Group by patient_id and iterate to get the latest non-null value for each feature\n",
    "grouped = df_sorted_by_date.groupby('patient_id')\n",
    "\n",
    "for patient_id, group in grouped:\n",
    "    patient_data = group.copy() # Make a copy to avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Fill in each feature from the most recent date backwards\n",
    "    for col in other_feature_cols_to_extract:\n",
    "        # Get the first non-null value for the current column\n",
    "        # This effectively checks rows from most recent to oldest\n",
    "        value = patient_data[col].dropna().iloc[0] if not patient_data[col].dropna().empty else pd.NA\n",
    "        final_other_features.loc[final_other_features['patient_id'] == patient_id, col] = value\n",
    "        \n",
    "print(\"Other feature extraction complete.\")\n",
    "\n",
    "# Step 7: Merge all processed dataframes\n",
    "# Combine the monthly HbA1c data with the patient's other most recent features.\n",
    "final_df = hba1c_pivot.merge(final_other_features, on='patient_id', how='left')\n",
    "\n",
    "# Step 8: Impute missing values for HbA1c columns only\n",
    "print(\"Imputing missing values for HbA1c columns...\")\n",
    "\n",
    "# Identify HbA1c columns\n",
    "hba1c_cols = [col for col in final_df.columns if col.startswith('hba1c_')]\n",
    "\n",
    "# Impute missing HbA1c values with the mean of available HbA1c values for that specific patient\n",
    "for index, row in final_df.iterrows():\n",
    "    patient_hba1c_values = row[hba1c_cols].dropna()\n",
    "    if not patient_hba1c_values.empty:\n",
    "        patient_hba1c_mean = round(patient_hba1c_values.mean(),2)\n",
    "        for col in hba1c_cols:\n",
    "            if pd.isna(row[col]):\n",
    "                final_df.loc[index, col] = patient_hba1c_mean\n",
    "\n",
    "print(\"Missing value imputation for HbA1c columns complete.\")\n",
    "\n",
    "\n",
    "# Step 9: Ensure the final output CSV has the specified column order\n",
    "# Define the order of 'other features' columns explicitly\n",
    "other_feature_cols = [\n",
    "    'age', 'gender', 'bmi', 'blood_pressure_systolic', 'blood_pressure_diastolic',\n",
    "    'blood_glucose_level', 'cci_score'\n",
    "]\n",
    "\n",
    "# Get the dynamically generated hba1c columns (re-get after potential imputation)\n",
    "hba1c_cols = [col for col in final_df.columns if col.startswith('hba1c_')]\n",
    "\n",
    "# Define the final desired column order\n",
    "final_column_order = ['patient_id'] + other_feature_cols + hba1c_cols\n",
    "\n",
    "# Reindex the DataFrame to enforce the column order\n",
    "final_df = final_df[final_column_order]\n",
    "\n",
    "# Step 10: Save the final processed data to a CSV file\n",
    "final_df.to_csv(\"10k_Imputed_Transformed_hba1c_data6_7.csv\", index=False)\n",
    "print(f\"Final data saved with shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fd0e5",
   "metadata": {},
   "source": [
    "# Without Fetching Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f76606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Define number of patients to limit the processing\n",
    "num_patients = 10000\n",
    "\n",
    "# Step 2: Load data from CSV file instead of SQL\n",
    "try:\n",
    "    df = pd.read_csv(\"10k_Imputed_Transformed_hba1c_data6_7.csv\")\n",
    "    print(\"Successfully loaded data from CSV file!\")\n",
    "except Exception as err:\n",
    "    print(f\"Error loading CSV file: {err}\")\n",
    "    exit()\n",
    "\n",
    "# Ensure correct data types after loading from CSV\n",
    "df['visit_date'] = pd.to_datetime(df['visit_date'])\n",
    "# Use 'Int64' for integer column that might contain NaNs (nulls from DB)\n",
    "df['cci_score'] = pd.to_numeric(df['cci_score'], errors='coerce').astype('Int64')\n",
    "# Ensure other numerical columns are also appropriate types if needed\n",
    "df['result_value'] = pd.to_numeric(df['result_value'], errors='coerce')\n",
    "df['bmi'] = pd.to_numeric(df['bmi'], errors='coerce')\n",
    "df['blood_pressure_systolic'] = pd.to_numeric(df['blood_pressure_systolic'], errors='coerce')\n",
    "df['blood_pressure_diastolic'] = pd.to_numeric(df['blood_pressure_diastolic'], errors='coerce')\n",
    "df['blood_glucose_level'] = pd.to_numeric(df['blood_glucose_level'], errors='coerce')\n",
    "\n",
    "# Step 3: Limit to the first `num_patients`\n",
    "# This ensures processing is limited to a specific number of unique patients.\n",
    "df = df.sort_values('patient_id')\n",
    "unique_patients = df['patient_id'].unique()[:num_patients]\n",
    "df = df[df['patient_id'].isin(unique_patients)].copy()\n",
    "\n",
    "# Step 4: Create monthly HbA1c pivot for the new date range (July 2023 - June 2025)\n",
    "# Filter the DataFrame to include only 'HbA1c' test results.\n",
    "hba1c_df = df[df['test_name'] == 'HbA1c'].copy()\n",
    "\n",
    "# Create a 'year_month' column for pivoting, in 'monthname_YYYY' format\n",
    "hba1c_df['year_month'] = hba1c_df['visit_date'].dt.strftime('%B_%Y').str.lower()\n",
    "\n",
    "# Sort data to identify the most recent 'result_value' for each patient within each year_month.\n",
    "hba1c_df = hba1c_df.sort_values(by=['patient_id', 'visit_date'], ascending=[True, False]) # Sort by visit_date descending to get latest\n",
    "\n",
    "# Remove duplicates within each patient-year_month group, keeping the most recent\n",
    "latest_hba1c_df = hba1c_df.drop_duplicates(subset=['patient_id', 'year_month'], keep='first')\n",
    "\n",
    "# Select only the required columns for pivoting\n",
    "latest_hba1c_df = latest_hba1c_df[['patient_id', 'year_month', 'result_value']]\n",
    "\n",
    "# Pivot the table to get 'year_month' as columns and HbA1c results as values.\n",
    "hba1c_pivot = latest_hba1c_df.pivot(index='patient_id', columns='year_month', values='result_value')\n",
    "\n",
    "# Generate a list of all expected 'monthname_YYYY' months from August 2023 to July 2025\n",
    "start_date = datetime(2023, 8, 1)\n",
    "end_date = datetime(2025, 7, 31)\n",
    "current_date = start_date\n",
    "expected_months = []\n",
    "while current_date <= end_date:\n",
    "    expected_months.append(current_date.strftime('%B_%Y').lower()) # Changed format to monthname_YYYY_lower\n",
    "    # Move to the next month\n",
    "    if current_date.month == 12:\n",
    "        current_date = current_date.replace(year=current_date.year + 1, month=1)\n",
    "    else:\n",
    "        current_date = current_date.replace(month=current_date.month + 1)\n",
    "\n",
    "# Reindex the pivot table to ensure all expected months are present, filling missing with NaN.\n",
    "hba1c_pivot = hba1c_pivot.reindex(columns=expected_months)\n",
    "\n",
    "# Rename the month columns to the desired format (hba1c_monthname_YYYY).\n",
    "hba1c_pivot.columns = [f\"hba1c_{col}\" for col in hba1c_pivot.columns]\n",
    "\n",
    "# Step 5: Extract other features by prioritizing the latest available month for each feature\n",
    "print(\"Extracting other features by prioritizing latest available month for each feature...\")\n",
    "\n",
    "other_feature_cols_to_extract = [\n",
    "    'age', 'gender', 'bmi', 'blood_pressure_systolic', 'blood_pressure_diastolic',\n",
    "    'blood_glucose_level', 'cci_score'\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame to store the final other features\n",
    "final_other_features = pd.DataFrame({'patient_id': unique_patients})\n",
    "\n",
    "# Sort the main DataFrame by patient_id and visit_date (descending)\n",
    "df_sorted_by_date = df.sort_values(by=['patient_id', 'visit_date'], ascending=[True, False])\n",
    "\n",
    "# Group by patient_id and iterate to get the latest non-null value for each feature\n",
    "grouped = df_sorted_by_date.groupby('patient_id')\n",
    "\n",
    "for patient_id, group in grouped:\n",
    "    patient_data = group.copy() # Make a copy to avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Fill in each feature from the most recent date backwards\n",
    "    for col in other_feature_cols_to_extract:\n",
    "        # Get the first non-null value for the current column\n",
    "        # This effectively checks rows from most recent to oldest\n",
    "        value = patient_data[col].dropna().iloc[0] if not patient_data[col].dropna().empty else pd.NA\n",
    "        final_other_features.loc[final_other_features['patient_id'] == patient_id, col] = value\n",
    "        \n",
    "print(\"Other feature extraction complete.\")\n",
    "\n",
    "# Step 6: Merge all processed dataframes\n",
    "# Combine the monthly HbA1c data with the patient's other most recent features.\n",
    "final_df = hba1c_pivot.merge(final_other_features, on='patient_id', how='left')\n",
    "\n",
    "# Step 7: Impute missing values for HbA1c columns only\n",
    "print(\"Imputing missing values for HbA1c columns...\")\n",
    "\n",
    "# Identify HbA1c columns\n",
    "hba1c_cols = [col for col in final_df.columns if col.startswith('hba1c_')]\n",
    "\n",
    "# Impute missing HbA1c values with the mean of available HbA1c values for that specific patient\n",
    "for index, row in final_df.iterrows():\n",
    "    patient_hba1c_values = row[hba1c_cols].dropna()\n",
    "    if not patient_hba1c_values.empty:\n",
    "        patient_hba1c_mean = round(patient_hba1c_values.mean(),2)\n",
    "        for col in hba1c_cols:\n",
    "            if pd.isna(row[col]):\n",
    "                final_df.loc[index, col] = patient_hba1c_mean\n",
    "\n",
    "print(\"Missing value imputation for HbA1c columns complete.\")\n",
    "\n",
    "# Step 8: Ensure the final output CSV has the specified column order\n",
    "# Define the order of 'other features' columns explicitly\n",
    "other_feature_cols = [\n",
    "    'age', 'gender', 'bmi', 'blood_pressure_systolic', 'blood_pressure_diastolic',\n",
    "    'blood_glucose_level', 'cci_score'\n",
    "]\n",
    "\n",
    "# Get the dynamically generated hba1c columns (re-get after potential imputation)\n",
    "hba1c_cols = [col for col in final_df.columns if col.startswith('hba1c_')]\n",
    "\n",
    "# Define the final desired column order\n",
    "final_column_order = ['patient_id'] + other_feature_cols + hba1c_cols\n",
    "\n",
    "# Reindex the DataFrame to enforce the column order\n",
    "final_df = final_df[final_column_order]\n",
    "\n",
    "# Step 9: Return the final processed data\n",
    "print(f\"Final data processed with shape: {final_df.shape}\")\n",
    "final_df  # This will return the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9a763",
   "metadata": {},
   "source": [
    "# Training and Testing/Validation for HbA1c\n",
    "with quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff095fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Models and Saving to Disk (2024-2025 Horizon) ---\n",
      "Saved point prediction model for hba1c_august_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_august_2024.pkl\n",
      "Saved lower quantile model for hba1c_august_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_august_2024_lower.pkl\n",
      "Saved upper quantile model for hba1c_august_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_august_2024_upper.pkl\n",
      "Saved point prediction model for hba1c_september_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_september_2024.pkl\n",
      "Saved lower quantile model for hba1c_september_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_september_2024_lower.pkl\n",
      "Saved upper quantile model for hba1c_september_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_september_2024_upper.pkl\n",
      "Saved point prediction model for hba1c_october_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_october_2024.pkl\n",
      "Saved lower quantile model for hba1c_october_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_october_2024_lower.pkl\n",
      "Saved upper quantile model for hba1c_october_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_october_2024_upper.pkl\n",
      "Saved point prediction model for hba1c_november_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_november_2024.pkl\n",
      "Saved lower quantile model for hba1c_november_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_november_2024_lower.pkl\n",
      "Saved upper quantile model for hba1c_november_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_november_2024_upper.pkl\n",
      "Saved point prediction model for hba1c_december_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_december_2024.pkl\n",
      "Saved lower quantile model for hba1c_december_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_december_2024_lower.pkl\n",
      "Saved upper quantile model for hba1c_december_2024 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_december_2024_upper.pkl\n",
      "Saved point prediction model for hba1c_january_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_january_2025.pkl\n",
      "Saved lower quantile model for hba1c_january_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_january_2025_lower.pkl\n",
      "Saved upper quantile model for hba1c_january_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_january_2025_upper.pkl\n",
      "Saved point prediction model for hba1c_february_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_february_2025.pkl\n",
      "Saved lower quantile model for hba1c_february_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_february_2025_lower.pkl\n",
      "Saved upper quantile model for hba1c_february_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_february_2025_upper.pkl\n",
      "Saved point prediction model for hba1c_march_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_march_2025.pkl\n",
      "Saved lower quantile model for hba1c_march_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_march_2025_lower.pkl\n",
      "Saved upper quantile model for hba1c_march_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_march_2025_upper.pkl\n",
      "Saved point prediction model for hba1c_april_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_april_2025.pkl\n",
      "Saved lower quantile model for hba1c_april_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_april_2025_lower.pkl\n",
      "Saved upper quantile model for hba1c_april_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_april_2025_upper.pkl\n",
      "Saved point prediction model for hba1c_may_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_may_2025.pkl\n",
      "Saved lower quantile model for hba1c_may_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_may_2025_lower.pkl\n",
      "Saved upper quantile model for hba1c_may_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_may_2025_upper.pkl\n",
      "Saved point prediction model for hba1c_june_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_june_2025.pkl\n",
      "Saved lower quantile model for hba1c_june_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_june_2025_lower.pkl\n",
      "Saved upper quantile model for hba1c_june_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_june_2025_upper.pkl\n",
      "Saved point prediction model for hba1c_july_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_july_2025.pkl\n",
      "Saved lower quantile model for hba1c_july_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_july_2025_lower.pkl\n",
      "Saved upper quantile model for hba1c_july_2025 to HbA1c_Trained_Models_with_Quantiles_Data6_7\\xgb_model_hba1c_july_2025_upper.pkl\n",
      "\n",
      "Training and Testing Performance (2024-2025 Horizon) ---\n",
      "\n",
      "Training Performance Across All Horizons ===\n",
      "[hba1c_august_2024] RMSE: 0.054, MAE: 0.041, MAPE: 0.57%\n",
      "[hba1c_september_2024] RMSE: 0.070, MAE: 0.050, MAPE: 0.70%\n",
      "[hba1c_october_2024] RMSE: 0.081, MAE: 0.057, MAPE: 0.81%\n",
      "[hba1c_november_2024] RMSE: 0.087, MAE: 0.062, MAPE: 0.90%\n",
      "[hba1c_december_2024] RMSE: 0.097, MAE: 0.069, MAPE: 0.94%\n",
      "[hba1c_january_2025] RMSE: 0.108, MAE: 0.077, MAPE: 1.02%\n",
      "[hba1c_february_2025] RMSE: 0.112, MAE: 0.080, MAPE: 1.07%\n",
      "[hba1c_march_2025] RMSE: 0.106, MAE: 0.077, MAPE: 1.05%\n",
      "[hba1c_april_2025] RMSE: 0.105, MAE: 0.076, MAPE: 1.06%\n",
      "[hba1c_may_2025] RMSE: 0.100, MAE: 0.073, MAPE: 1.02%\n",
      "[hba1c_june_2025] RMSE: 0.089, MAE: 0.065, MAPE: 0.93%\n",
      "[hba1c_july_2025] RMSE: 0.078, MAE: 0.059, MAPE: 0.88%\n",
      "\n",
      "Testing Performance Across All Horizons ===\n",
      "[hba1c_august_2024] RMSE: 0.200, MAE: 0.090, MAPE: 1.18%\n",
      "[hba1c_september_2024] RMSE: 0.250, MAE: 0.110, MAPE: 1.46%\n",
      "[hba1c_october_2024] RMSE: 0.300, MAE: 0.130, MAPE: 1.66%\n",
      "[hba1c_november_2024] RMSE: 0.290, MAE: 0.140, MAPE: 1.83%\n",
      "[hba1c_december_2024] RMSE: 0.310, MAE: 0.150, MAPE: 1.96%\n",
      "[hba1c_january_2025] RMSE: 0.310, MAE: 0.170, MAPE: 2.09%\n",
      "[hba1c_february_2025] RMSE: 0.320, MAE: 0.170, MAPE: 2.10%\n",
      "[hba1c_march_2025] RMSE: 0.280, MAE: 0.160, MAPE: 2.05%\n",
      "[hba1c_april_2025] RMSE: 0.270, MAE: 0.150, MAPE: 1.99%\n",
      "[hba1c_may_2025] RMSE: 0.260, MAE: 0.150, MAPE: 1.94%\n",
      "[hba1c_june_2025] RMSE: 0.220, MAE: 0.130, MAPE: 1.83%\n",
      "[hba1c_july_2025] RMSE: 0.160, MAE: 0.120, MAPE: 1.67%\n",
      "\n",
      "Actual vs. Predicted for 2024-2025 saved to: HbA1c_2024-2025_Actual_vs_Predicted_Data6_7.csv\n",
      "\n",
      "Training process complete: Point prediction models trained, and lower/upper quantile models trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "import calendar\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = \"10k_Imputed_Transformed_hba1c_data6_7.csv\"\n",
    "MODELS_DIR = \"HbA1c_Trained_Models_with_Quantiles_Data6_7\" # Directory to save trained models\n",
    "PREDICTIONS_ACTUAL_VS_PREDICTED_FILE = \"HbA1c_2024-2025_Actual_vs_Predicted_Data6_7.csv\"\n",
    "TESTING_METRICS_FILE = \"HbA1c_Testing_Metrics_Data6_7.csv\"\n",
    "\n",
    "# --- Define the base year for lag features and target horizons ---\n",
    "# Adjust these years to easily change the timeframes\n",
    "LAG_FEATURE_START_YEAR = 2023 # Lag features will start from August of this year\n",
    "LAG_FEATURE_END_YEAR = 2024   # Lag features will end in July of this year\n",
    "\n",
    "TARGET_HORIZON_START_YEAR = 2024 # Target horizons will start from August of this year\n",
    "TARGET_HORIZON_END_YEAR = 2025   # Target horizons will end in July of this year\n",
    "\n",
    "\n",
    "# Create directory for models if it doesn't exist\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# --- NEW: Perform gender encoding ONCE on the full DataFrame before the loop ---\n",
    "df['gender_encoded'] = df['gender'].astype('category').cat.codes\n",
    "\n",
    "# Define static features for the model (now directly using 'gender_encoded')\n",
    "static_features_for_model = [\n",
    "    'age', 'gender_encoded', 'bmi', 'blood_pressure_systolic', 'blood_pressure_diastolic',\n",
    "    'blood_glucose_level', 'cci_score'\n",
    "]\n",
    "\n",
    "# --- Dynamic generation of Lagged HbA1c features (August to July) ---\n",
    "lag_features_for_training = []\n",
    "# Loop for the starting year (August to December)\n",
    "for month_num in range(8, 13):\n",
    "    month_name = calendar.month_name[month_num].lower()\n",
    "    lag_features_for_training.append(f\"hba1c_{month_name}_{LAG_FEATURE_START_YEAR}\")\n",
    "\n",
    "# Loop for the ending year (January to July)\n",
    "for month_num in range(1, 8):\n",
    "    month_name = calendar.month_name[month_num].lower()\n",
    "    lag_features_for_training.append(f\"hba1c_{month_name}_{LAG_FEATURE_END_YEAR}\")\n",
    "\n",
    "# --- Dynamic generation of Target HbA1c columns (August to July) ---\n",
    "target_cols = []\n",
    "# Loop for the starting year (August to December)\n",
    "for month_num in range(8, 13):\n",
    "    month_name = calendar.month_name[month_num].lower()\n",
    "    target_cols.append(f\"hba1c_{month_name}_{TARGET_HORIZON_START_YEAR}\")\n",
    "\n",
    "# Loop for the ending year (January to July)\n",
    "for month_num in range(1, 8):\n",
    "    month_name = calendar.month_name[month_num].lower()\n",
    "    target_cols.append(f\"hba1c_{month_name}_{TARGET_HORIZON_END_YEAR}\")\n",
    "\n",
    "\n",
    "# Combine static and lag features to form the complete feature set for training\n",
    "feature_cols_training = static_features_for_model + lag_features_for_training\n",
    "\n",
    "# Handle missing values for features. Numeric NaNs are filled with 0.\n",
    "df[feature_cols_training] = df[feature_cols_training].fillna(0)\n",
    "\n",
    "# Initialize a DataFrame to store actual vs. predicted values for all patients\n",
    "result_df_all_horizons = df[['patient_id']].copy()\n",
    "\n",
    "# Lists to store training and testing metrics separately for console printing and saving\n",
    "training_metrics_all_horizons = []\n",
    "testing_metrics_all_horizons = []\n",
    "\n",
    "# Function to calculate MAPE (Mean Absolute Percentage Error)\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# --- Training and Saving Models for the defined horizons ---\n",
    "print(f\"--- Training Models and Saving to Disk ({TARGET_HORIZON_START_YEAR}-{TARGET_HORIZON_END_YEAR} Horizon) ---\")\n",
    "for h, target_col in enumerate(target_cols, start=1):\n",
    "    # Skip if the target column is not present in the DataFrame\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Warning: {target_col} missing in data, skipping this forecast horizon.\")\n",
    "        continue\n",
    "\n",
    "    # Create a temporary DataFrame containing only non-null values for the current target.\n",
    "    df_h = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "    # Use the pre-defined feature_cols_training which already contains 'gender_encoded'.\n",
    "    current_feature_cols = feature_cols_training\n",
    "\n",
    "    # Define features (X) and target (y) for the current forecast horizon\n",
    "    X = df_h[current_feature_cols]\n",
    "    y = df_h[target_col]\n",
    "\n",
    "    # Split the data into training and testing sets (80% training, 20% testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # --- Train and Save the main XGBoost Regressor model (your original logic) ---\n",
    "    # This model uses 'reg:squarederror' and provides the point estimate.\n",
    "    # We will continue to save it with its original filename.\n",
    "    model_point = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    model_point.fit(X_train, y_train)\n",
    "\n",
    "    model_filename_point = os.path.join(MODELS_DIR, f\"xgb_model_{target_col}.pkl\")\n",
    "    with open(model_filename_point, 'wb') as f:\n",
    "        pickle.dump(model_point, f)\n",
    "    print(f\"Saved point prediction model for {target_col} to {model_filename_point}\")\n",
    "\n",
    "\n",
    "    # --- Train and Save the Lower Quantile XGBoost model ---\n",
    "    # Using quantile_alpha=0.025 for the 2.5th percentile (for a 95% interval lower bound)\n",
    "    model_lower_quantile = xgb.XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=0.025,\n",
    "        random_state=42,\n",
    "        # Keep consistent hyperparameters with your main model\n",
    "        n_estimators=model_point.n_estimators,\n",
    "        learning_rate=model_point.learning_rate,\n",
    "        max_depth=model_point.max_depth,\n",
    "        # ... include any other hyperparameters from your model_point if set\n",
    "    )\n",
    "    model_lower_quantile.fit(X_train, y_train)\n",
    "\n",
    "    model_filename_lower = os.path.join(MODELS_DIR, f\"xgb_model_{target_col}_lower.pkl\")\n",
    "    with open(model_filename_lower, 'wb') as f:\n",
    "        pickle.dump(model_lower_quantile, f)\n",
    "    print(f\"Saved lower quantile model for {target_col} to {model_filename_lower}\")\n",
    "\n",
    "\n",
    "    # --- Train and Save the Upper Quantile XGBoost model ---\n",
    "    # Using quantile_alpha=0.975 for the 97.5th percentile (for a 95% interval upper bound)\n",
    "    model_upper_quantile = xgb.XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=0.975,\n",
    "        random_state=42,\n",
    "        # Keep consistent hyperparameters with your main model\n",
    "        n_estimators=model_point.n_estimators,\n",
    "        learning_rate=model_point.learning_rate,\n",
    "        max_depth=model_point.max_depth,\n",
    "        # ... include any other hyperparameters from your model_point if set\n",
    "    )\n",
    "    model_upper_quantile.fit(X_train, y_train)\n",
    "\n",
    "    model_filename_upper = os.path.join(MODELS_DIR, f\"xgb_model_{target_col}_upper.pkl\")\n",
    "    with open(model_filename_upper, 'wb') as f:\n",
    "        pickle.dump(model_upper_quantile, f)\n",
    "    print(f\"Saved upper quantile model for {target_col} to {model_filename_upper}\")\n",
    "\n",
    "\n",
    "    # --- Evaluate Training Performance (using the point prediction model) ---\n",
    "    y_train_pred = model_point.predict(X_train)\n",
    "    y_train_pred = np.round(y_train_pred, 2)\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mape_train = calculate_mape(y_train, y_train_pred)\n",
    "\n",
    "    training_metrics_all_horizons.append({\n",
    "        \"Horizon_Month\": h,\n",
    "        \"Target\": target_col,\n",
    "        \"RMSE\": rmse_train,\n",
    "        \"MAE\": mae_train,\n",
    "        \"MAPE (%)\": mape_train\n",
    "    })\n",
    "\n",
    "    # --- Evaluate Testing Performance (using the point prediction model) ---\n",
    "    y_test_pred = model_point.predict(X_test)\n",
    "    y_test_pred = np.round(y_test_pred, 2)\n",
    "\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    mape_test = calculate_mape(y_test, y_test_pred)\n",
    "\n",
    "    testing_metrics_all_horizons.append({\n",
    "        \"Target\": target_col,\n",
    "        \"RMSE\": round(rmse_test,2),\n",
    "        \"MAE\": round(mae_test,2),\n",
    "        \"MAPE (%)\": round(mape_test,2)\n",
    "    })\n",
    "\n",
    "    testing_metrics_df = pd.DataFrame(testing_metrics_all_horizons)\n",
    "    testing_metrics_df.to_csv(TESTING_METRICS_FILE, index=False)\n",
    "\n",
    "    # --- Prepare data for actual vs. predicted CSV for current target (using the point prediction model) ---\n",
    "    y_full_h_pred = model_point.predict(X)\n",
    "    y_full_h_pred = np.round(y_full_h_pred, 2)\n",
    "\n",
    "    result_df_all_horizons = result_df_all_horizons.merge(\n",
    "        pd.DataFrame({\n",
    "            'patient_id': df_h['patient_id'],\n",
    "            f\"actual_{target_col}\": y,\n",
    "            f\"predicted_{target_col}\": y_full_h_pred\n",
    "        }),\n",
    "        on='patient_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "print(f\"\\nTraining and Testing Performance ({TARGET_HORIZON_START_YEAR}-{TARGET_HORIZON_END_YEAR} Horizon) ---\")\n",
    "print(\"\\nTraining Performance Across All Horizons ===\")\n",
    "for m in training_metrics_all_horizons:\n",
    "    print(f\"[{m['Target']}] RMSE: {m['RMSE']:.3f}, MAE: {m['MAE']:.3f}, MAPE: {m['MAPE (%)']:.2f}%\")\n",
    "\n",
    "print(\"\\nTesting Performance Across All Horizons ===\")\n",
    "for m in testing_metrics_all_horizons:\n",
    "    print(f\"[{m['Target']}] RMSE: {m['RMSE']:.3f}, MAE: {m['MAE']:.3f}, MAPE: {m['MAPE (%)']:.2f}%\")\n",
    "\n",
    "# Save the actual vs. predicted results\n",
    "result_df_all_horizons.to_csv(PREDICTIONS_ACTUAL_VS_PREDICTED_FILE, index=False)\n",
    "print(f\"\\nActual vs. Predicted for {TARGET_HORIZON_START_YEAR}-{TARGET_HORIZON_END_YEAR} saved to: {PREDICTIONS_ACTUAL_VS_PREDICTED_FILE}\")\n",
    "\n",
    "print(\"\\nTraining process complete: Point prediction models trained, and lower/upper quantile models trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811e8b3",
   "metadata": {},
   "source": [
    "# Prediction HbA1c 2025-2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6d61df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Predictions for 2025-08 to 2026-07 Horizon ---\n",
      "Loaded point prediction model 'xgb_model_hba1c_august_2024.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_august_2024_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_august_2024_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_september_2024.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_september_2024_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_september_2024_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_october_2024.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_october_2024_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_october_2024_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_november_2024.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_november_2024_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_november_2024_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_december_2024.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_december_2024_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_december_2024_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_january_2025.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_january_2025_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_january_2025_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_february_2025.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_february_2025_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_february_2025_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_march_2025.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_march_2025_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_march_2025_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_april_2025.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_april_2025_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_april_2025_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_may_2025.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_may_2025_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_may_2025_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_june_2025.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_june_2025_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_june_2025_upper.pkl'.\n",
      "Loaded point prediction model 'xgb_model_hba1c_july_2025.pkl'.\n",
      "Loaded lower quantile model 'xgb_model_hba1c_july_2025_lower.pkl'.\n",
      "Loaded upper quantile model 'xgb_model_hba1c_july_2025_upper.pkl'.\n",
      "\n",
      "Completed generating wide-format predictions. Now transforming to long format...\n",
      "Extracted 120000 actuals records.\n",
      "Generated 360000 predicted records.\n",
      "\n",
      "Final combined actuals and predictions saved to: HbA1c_2024-2026_Normalized_Data6_7.csv\n",
      "\n",
      "Output generation complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import calendar\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Configuration for Prediction ---\n",
    "DATA_FILE = \"10k_Imputed_Transformed_hba1c_data6_7.csv\"\n",
    "MODELS_DIR = \"HbA1c_Trained_Models_with_Quantiles_Data6_7\" # Directory where trained models are saved\n",
    "FINAL_OUTPUT_FILE = \"HbA1c_2024-2026_Normalized_Data6_7.csv\"\n",
    "\n",
    "# --- Define the desired future prediction horizon ---\n",
    "PREDICTION_FORECAST_START_YEAR = 2025\n",
    "PREDICTION_FORECAST_END_YEAR = 2026\n",
    "\n",
    "# --- Define the actuals historical period to include in output ---\n",
    "ACTUALS_START_DATE = datetime(2024, 8, 1)\n",
    "ACTUALS_END_DATE = datetime(2025, 7, 1) # Ends on the 1st of July 2025 for actuals for July 2025\n",
    "\n",
    "# --- Month Names Helper ---\n",
    "FULL_MONTH_NAMES = [calendar.month_name[i].lower() for i in range(1, 13)] # ['january', ..., 'december']\n",
    "\n",
    "# --- Derived Years for Dynamic Column Generation ---\n",
    "TRAINED_MODEL_TARGET_START_YEAR = PREDICTION_FORECAST_START_YEAR - 1\n",
    "TRAINED_MODEL_TARGET_END_YEAR = PREDICTION_FORECAST_END_YEAR - 1\n",
    "\n",
    "MODEL_TEMPLATE_LAG_START_YEAR = TRAINED_MODEL_TARGET_START_YEAR - 1\n",
    "MODEL_TEMPLATE_LAG_END_YEAR = TRAINED_MODEL_TARGET_END_YEAR - 1\n",
    "\n",
    "CURRENT_INPUT_LAG_START_YEAR = PREDICTION_FORECAST_START_YEAR - 1\n",
    "CURRENT_INPUT_LAG_END_YEAR = PREDICTION_FORECAST_END_YEAR - 1\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "try:\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{DATA_FILE}' not found. Please ensure the dataset file is in the specified path.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Ensure gender is encoded consistently as it was during training\n",
    "if 'gender' in df.columns and 'gender_encoded' not in df.columns:\n",
    "    df['gender_encoded'] = df['gender'].astype('category').cat.codes\n",
    "elif 'gender_encoded' not in df.columns:\n",
    "    print(\"Warning: 'gender' column not found for encoding. Ensure 'gender_encoded' is present or handled.\")\n",
    "\n",
    "\n",
    "# Define static features (must match what models were trained on)\n",
    "static_features_for_model = [\n",
    "    'age', 'gender_encoded', 'bmi', 'blood_pressure_systolic', 'blood_pressure_diastolic',\n",
    "    'blood_glucose_level', 'cci_score'\n",
    "]\n",
    "\n",
    "# --- Dynamic Generation of Lagged HbA1c features for the MODEL TEMPLATE ---\n",
    "lag_features_model_template = (\n",
    "    [f\"hba1c_{FULL_MONTH_NAMES[month-1]}_{MODEL_TEMPLATE_LAG_START_YEAR}\" for month in range(8, 13)] + # Aug-Dec\n",
    "    [f\"hba1c_{FULL_MONTH_NAMES[month-1]}_{MODEL_TEMPLATE_LAG_END_YEAR}\" for month in range(1, 8)]      # Jan-Jul\n",
    ")\n",
    "\n",
    "# --- Dynamic Generation of Lagged HbA1c features for CURRENT PREDICTION INPUT ---\n",
    "lag_features_current_input = (\n",
    "    [f\"hba1c_{FULL_MONTH_NAMES[month-1]}_{CURRENT_INPUT_LAG_START_YEAR}\" for month in range(8, 13)] + # Aug-Dec\n",
    "    [f\"hba1c_{FULL_MONTH_NAMES[month-1]}_{CURRENT_INPUT_LAG_END_YEAR}\" for month in range(1, 8)]      # Jan-Jul\n",
    ")\n",
    "\n",
    "# Combine static and lagged features to form the complete list of features for model input\n",
    "feature_cols_for_model_input = static_features_for_model + lag_features_model_template\n",
    "\n",
    "\n",
    "# Handle missing values for features used in prediction (fill with 0, consistent with training)\n",
    "all_required_input_cols = static_features_for_model + lag_features_current_input\n",
    "for col in all_required_input_cols:\n",
    "    if col not in df.columns:\n",
    "        print(f\"Error: Required feature column '{col}' for prediction is missing in the input file '{DATA_FILE}'. Please ensure your data file is updated.\")\n",
    "        exit()\n",
    "\n",
    "df[all_required_input_cols] = df[all_required_input_cols].fillna(0)\n",
    "\n",
    "\n",
    "# --- Dynamic Generation of the target columns for the future prediction horizon ---\n",
    "target_cols_future_prediction = (\n",
    "    [f\"hba1c_{FULL_MONTH_NAMES[month-1]}_{PREDICTION_FORECAST_START_YEAR}\" for month in range(8, 13)] + # Aug-Dec\n",
    "    [f\"hba1c_{FULL_MONTH_NAMES[month-1]}_{PREDICTION_FORECAST_END_YEAR}\" for month in range(1, 8)]      # Jan-Jul\n",
    ")\n",
    "\n",
    "# --- Dynamic Generation of the corresponding target columns that were used to train the models ---\n",
    "trained_model_target_cols = (\n",
    "    [f\"hba1c_{FULL_MONTH_NAMES[month-1]}_{TRAINED_MODEL_TARGET_START_YEAR}\" for month in range(8, 13)] + # Aug-Dec\n",
    "    [f\"hba1c_{FULL_MONTH_NAMES[month-1]}_{TRAINED_MODEL_TARGET_END_YEAR}\" for month in range(1, 8)]      # Jan-Jul\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize a DataFrame to store the predictions temporarily in wide format\n",
    "# This will be converted to long format later\n",
    "temp_wide_predictions_df = df[['patient_id']].copy()\n",
    "\n",
    "print(f\"--- Generating Predictions for {PREDICTION_FORECAST_START_YEAR}-08 to {PREDICTION_FORECAST_END_YEAR}-07 Horizon ---\")\n",
    "\n",
    "# Iterate through the future target months and make predictions\n",
    "for i, current_pred_target in enumerate(target_cols_future_prediction):\n",
    "    model_to_load_name = trained_model_target_cols[i]\n",
    "    \n",
    "    # Define filenames for all three models (point, lower quantile, upper quantile)\n",
    "    model_filename_point = os.path.join(MODELS_DIR, f\"xgb_model_{model_to_load_name}.pkl\")\n",
    "    model_filename_lower = os.path.join(MODELS_DIR, f\"xgb_model_{model_to_load_name}_lower.pkl\")\n",
    "    model_filename_upper = os.path.join(MODELS_DIR, f\"xgb_model_{model_to_load_name}_upper.pkl\")\n",
    "\n",
    "    # Define output column names for point prediction and its bounds in the temp wide df\n",
    "    output_col_name_point = f\"predicted_{current_pred_target}\"\n",
    "    output_col_name_lower = f\"predicted_{current_pred_target}_lower\"\n",
    "    output_col_name_upper = f\"predicted_{current_pred_target}_upper\"\n",
    "\n",
    "    # Initialize columns to NaN in case any model or prediction fails for this month\n",
    "    temp_wide_predictions_df[output_col_name_point] = np.nan\n",
    "    temp_wide_predictions_df[output_col_name_lower] = np.nan\n",
    "    temp_wide_predictions_df[output_col_name_upper] = np.nan\n",
    "\n",
    "    # Initialize loaded models\n",
    "    loaded_model_point = None\n",
    "    loaded_model_lower = None\n",
    "    loaded_model_upper = None\n",
    "\n",
    "    try:\n",
    "        # Load the point prediction model\n",
    "        if os.path.exists(model_filename_point):\n",
    "            with open(model_filename_point, 'rb') as f:\n",
    "                loaded_model_point = pickle.load(f)\n",
    "            print(f\"Loaded point prediction model '{os.path.basename(model_filename_point)}'.\")\n",
    "        else:\n",
    "            print(f\"Error: Point prediction model file '{model_filename_point}' not found. Cannot predict '{current_pred_target}'. Skipping this month.\")\n",
    "            continue # Skip to the next target month if the primary model is missing\n",
    "\n",
    "        # Load the lower quantile model\n",
    "        if os.path.exists(model_filename_lower):\n",
    "            with open(model_filename_lower, 'rb') as f:\n",
    "                loaded_model_lower = pickle.load(f)\n",
    "            print(f\"Loaded lower quantile model '{os.path.basename(model_filename_lower)}'.\")\n",
    "        else:\n",
    "            print(f\"Warning: Lower quantile model file '{model_filename_lower}' not found. Lower bound for '{current_pred_target}' will be NaN.\")\n",
    "\n",
    "        # Load the upper quantile model\n",
    "        if os.path.exists(model_filename_upper):\n",
    "            with open(model_filename_upper, 'rb') as f:\n",
    "                loaded_model_upper = pickle.load(f)\n",
    "            print(f\"Loaded upper quantile model '{os.path.basename(model_filename_upper)}'.\")\n",
    "        else:\n",
    "            print(f\"Warning: Upper quantile model file '{model_filename_upper}' not found. Upper bound for '{current_pred_target}' will be NaN.\")\n",
    "\n",
    "        # Prepare the input features for prediction.\n",
    "        X_predict_data = df[static_features_for_model].copy()\n",
    "\n",
    "        # Create a mapping for the lagged features from their current input names to their model template names\n",
    "        lag_feature_mapping = {\n",
    "            lag_current: lag_template\n",
    "            for lag_current, lag_template in zip(lag_features_current_input, lag_features_model_template)\n",
    "        }\n",
    "\n",
    "        # Select the current input lagged features and rename them to match the model's expected input\n",
    "        X_predict_lags = df[lag_features_current_input].rename(columns=lag_feature_mapping)\n",
    "\n",
    "        # Combine static features and renamed lagged features\n",
    "        X_predict = pd.concat([X_predict_data, X_predict_lags], axis=1)\n",
    "\n",
    "        # Ensure the order of columns in X_predict matches the original training order\n",
    "        X_predict = X_predict[feature_cols_for_model_input]\n",
    "\n",
    "        # --- Make predictions using all three loaded models ---\n",
    "        # Point prediction (values are rounded here to 2 decimal places)\n",
    "        y_pred_point = loaded_model_point.predict(X_predict)\n",
    "        y_pred_point = np.round(y_pred_point, 2)\n",
    "        temp_wide_predictions_df[output_col_name_point] = y_pred_point\n",
    "\n",
    "        # Lower bound prediction\n",
    "        if loaded_model_lower:\n",
    "            y_pred_lower = loaded_model_lower.predict(X_predict)\n",
    "            y_pred_lower = np.round(y_pred_lower, 2) # Ensure rounding at this stage\n",
    "            # Post-processing for lower bound:\n",
    "            # 1. HbA1c cannot be negative.\n",
    "            # 2. Lower bound should not cross above the point prediction.\n",
    "            y_pred_lower = np.maximum(0, np.minimum(y_pred_lower, y_pred_point))\n",
    "            temp_wide_predictions_df[output_col_name_lower] = y_pred_lower\n",
    "        # Else: it's already NaN from initialization\n",
    "\n",
    "        # Upper bound prediction\n",
    "        if loaded_model_upper:\n",
    "            y_pred_upper = loaded_model_upper.predict(X_predict)\n",
    "            y_pred_upper = np.round(y_pred_upper, 2) # Ensure rounding at this stage\n",
    "            # Post-processing for upper bound:\n",
    "            # 1. Upper bound should not cross below the point prediction.\n",
    "            y_pred_upper = np.maximum(y_pred_upper, y_pred_point)\n",
    "            temp_wide_predictions_df[output_col_name_upper] = y_pred_upper\n",
    "        # Else: it's already NaN from initialization\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while predicting '{current_pred_target}': {e}. All predictions for this month will be NaN.\")\n",
    "        continue # Skip to the next target month\n",
    "\n",
    "print(f\"\\nCompleted generating wide-format predictions. Now transforming to long format...\")\n",
    "\n",
    "# --- Part 1: Extract Actuals in Long Format (2024-08-01 to 2025-07-31) ---\n",
    "actual_cols_to_melt = []\n",
    "current_date = ACTUALS_START_DATE\n",
    "while current_date <= ACTUALS_END_DATE:\n",
    "    month_name = FULL_MONTH_NAMES[current_date.month - 1]\n",
    "    col_name = f\"hba1c_{month_name}_{current_date.year}\"\n",
    "    if col_name in df.columns:\n",
    "        actual_cols_to_melt.append(col_name)\n",
    "    else:\n",
    "        print(f\"Warning: Actual column '{col_name}' missing from input data. It will not be included in the output actuals.\")\n",
    "    \n",
    "    # Move to the next month\n",
    "    if current_date.month == 12:\n",
    "        current_date = current_date.replace(year=current_date.year + 1, month=1)\n",
    "    else:\n",
    "        current_date = current_date.replace(month=current_date.month + 1)\n",
    "\n",
    "actuals_df_long = pd.DataFrame()\n",
    "if actual_cols_to_melt:\n",
    "    # Melt the actual columns\n",
    "    actuals_df_long = df[['patient_id'] + actual_cols_to_melt].melt(\n",
    "        id_vars=['patient_id'],\n",
    "        var_name='month_year_hba1c',\n",
    "        value_name='value'\n",
    "    )\n",
    "    actuals_df_long['metric_type'] = 'actual'\n",
    "\n",
    "    # Extract month and year from 'month_year_hba1c' to create 'Date' column in 'YYYY-MM-DD' format\n",
    "    actuals_df_long['Date'] = actuals_df_long['month_year_hba1c'].apply(lambda x: datetime.strptime(x.split('_')[-2].capitalize() + x.split('_')[-1], '%B%Y').strftime('%Y-%m-%d'))\n",
    "    actuals_df_long = actuals_df_long[['patient_id', 'Date', 'metric_type', 'value']] # Changed 'month' to 'Date' here\n",
    "    # Round actual values to 2 decimal places as well for consistency in the final output\n",
    "    actuals_df_long['value'] = actuals_df_long['value'].round(2)\n",
    "    print(f\"Extracted {len(actuals_df_long)} actuals records.\")\n",
    "else:\n",
    "    print(\"No actuals columns found for the specified period. Actuals will not be included.\")\n",
    "\n",
    "\n",
    "# --- Part 2: Convert Predicted Data to Long Format ---\n",
    "all_predicted_rows_list = []\n",
    "\n",
    "# Loop through each predicted target month\n",
    "for current_pred_target in target_cols_future_prediction:\n",
    "    output_col_name_point = f\"predicted_{current_pred_target}\"\n",
    "    output_col_name_lower = f\"predicted_{current_pred_target}_lower\"\n",
    "    output_col_name_upper = f\"predicted_{current_pred_target}_upper\"\n",
    "\n",
    "    # Extract month and year from the target column name\n",
    "    parts = current_pred_target.split('_')\n",
    "    month_name = parts[1]\n",
    "    year = int(parts[2])\n",
    "    month_num = FULL_MONTH_NAMES.index(month_name) + 1\n",
    "    date_str = f\"{year}-{month_num:02d}-01\" # This will be the 'Date' value\n",
    "\n",
    "    # Get the data for the current month across all patients\n",
    "    temp_data = temp_wide_predictions_df[['patient_id', output_col_name_point, output_col_name_lower, output_col_name_upper]].copy()\n",
    "\n",
    "    # Rename columns for melting\n",
    "    temp_data.rename(columns={\n",
    "        output_col_name_point: 'predicted',\n",
    "        output_col_name_lower: 'lower_bound',\n",
    "        output_col_name_upper: 'upper_bound'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Melt this month's data\n",
    "    melted_month_data = temp_data.melt(\n",
    "        id_vars=['patient_id'],\n",
    "        var_name='metric_type',\n",
    "        value_name='value'\n",
    "    )\n",
    "    melted_month_data['Date'] = date_str # Changed 'month' to 'Date' here\n",
    "    \n",
    "    # Append to the list\n",
    "    all_predicted_rows_list.append(melted_month_data)\n",
    "\n",
    "predictions_df_long = pd.DataFrame()\n",
    "if all_predicted_rows_list:\n",
    "    predictions_df_long = pd.concat(all_predicted_rows_list, ignore_index=True)\n",
    "    print(f\"Generated {len(predictions_df_long)} predicted records.\")\n",
    "else:\n",
    "    print(\"No predictions were generated. Predicted data will not be included.\")\n",
    "\n",
    "\n",
    "# --- Part 3: Combine and Save the Final Output ---\n",
    "final_output_df = pd.concat([actuals_df_long, predictions_df_long], ignore_index=True)\n",
    "\n",
    "# Convert 'Date' column to datetime for proper chronological sorting\n",
    "final_output_df['Date'] = pd.to_datetime(final_output_df['Date']) # Changed 'month' to 'Date' here\n",
    "\n",
    "# Define the custom order for 'metric_type' as requested\n",
    "metric_type_order = ['actual', 'predicted', 'lower_bound', 'upper_bound']\n",
    "final_output_df['metric_type'] = pd.Categorical(final_output_df['metric_type'], categories=metric_type_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by patient_id, Date, and then the custom metric_type order\n",
    "final_output_df.sort_values(by=['patient_id', 'Date', 'metric_type'], inplace=True) # Changed 'month' to 'Date' here\n",
    "\n",
    "# Convert 'Date' column back to string format 'YYYY-MM-DD'\n",
    "final_output_df['Date'] = final_output_df['Date'].dt.strftime('%Y-%m-%d') # Changed 'month' to 'Date' here\n",
    "\n",
    "# Save the final DataFrame to CSV\n",
    "final_output_df.to_csv(FINAL_OUTPUT_FILE, index=False, float_format=\"%.2f\")\n",
    "print(f\"\\nFinal combined actuals and predictions saved to: {FINAL_OUTPUT_FILE}\")\n",
    "\n",
    "print(\"\\nOutput generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
